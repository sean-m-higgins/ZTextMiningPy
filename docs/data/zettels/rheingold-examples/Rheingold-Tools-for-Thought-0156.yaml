title: 01.01.09-Rheingold-Tools-for-Thought-0156
note: |
  The limit imposed by nature is concerned only with the limit of the communication channel.
  As long as there is a channel, no matter how noisy, a code can be devised to transmit any message with any degree of certainty.
  Entropy is a measure of the relationship between the complexity of the code and the degree of certainty.
  These theorems meant a lot to radio and telephone engineers, and made color television as well as broadcasts from the moon possible, but Shannon stated them in a way that demonstrated their universality beyond the domain of electrical engineering.
  The key to life itself, in fact, turned out to be a matter of information, as the world learned five years later, when that young physicist-turned-biologist who had attended Schrdinger's lecture, Francis Crick, teamed up with James Watson to decipher the molecular genetic coding of the DNA helix.
  Scientifically, and on the level of consciousness, people seemed to jump rather too quickly to make the transition from an energy-based metaphor of the universe to an information model.
  The rush to generalize information theory to all sorts of scientific areas, some of them of dubious scientific merit, led Shannon to decry this "bandwagon effect," remarking that information theory "has perhaps ballooned to an importance beyond its actual accomplishments.
  Seldom do more than a few of nature's secrets give way at one time."
  Despite Shannon's disclaimer, information- and communication-based models have proved to be enormously useful in the sciences because so many important phenomena can be seen in terms of messages.
  Human bodies can be better understood as complex communication networks than as clockwork-like machines.
tags:
- Core Text
- Computing History
- Rheingold 0156
cite:
  bibkey: Rheingold_ToolsThoughtHistory_2000
  page: PDF eBook

